{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment2_PartA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "venva",
      "display_name": "venvA",
      "language": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7bf35725e3c48f6a048ea6c24a1f45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fa92da97ab1a44979c7bf3ef5c58defb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b30f66d09ce74efcb9384a7cb41cfed7",
              "IPY_MODEL_ca0c49807c11404f8735bdc3b52d29f3"
            ]
          }
        },
        "fa92da97ab1a44979c7bf3ef5c58defb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b30f66d09ce74efcb9384a7cb41cfed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_af7da6bc2e984a129374e591262209ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.68MB of 2.68MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c76c920cc36428494cc0c236553058b"
          }
        },
        "ca0c49807c11404f8735bdc3b52d29f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9c699f7a6db416caa141c789361e4da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8f25d64ad4c42cd8bfe254fab281ee3"
          }
        },
        "af7da6bc2e984a129374e591262209ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c76c920cc36428494cc0c236553058b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9c699f7a6db416caa141c789361e4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8f25d64ad4c42cd8bfe254fab281ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "734ecba5fb41499291f655ace9f9828f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4783c1e69b734c8a806ad7578e3a7253",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f6cbf123491645ad9f348c1c70a3ebcc",
              "IPY_MODEL_35872fd3056648109211cbeedd73ba70"
            ]
          }
        },
        "4783c1e69b734c8a806ad7578e3a7253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6cbf123491645ad9f348c1c70a3ebcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_7c1226ec8e144d768d18d8028399658f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.11MB of 1.11MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3caf048d4e24c3aadb9ba6f112d375d"
          }
        },
        "35872fd3056648109211cbeedd73ba70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9da06116257d40478134e605d95a5a9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_876bd581e480419da6ca48a23d325ff1"
          }
        },
        "7c1226ec8e144d768d18d8028399658f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3caf048d4e24c3aadb9ba6f112d375d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9da06116257d40478134e605d95a5a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "876bd581e480419da6ca48a23d325ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0EP3CrxEbJi",
        "outputId": "17a8b33d-ffc4-44f4-96ca-1c4f153f9ee5"
      },
      "source": [
        "%%time\n",
        "# The below code will download the zipped folder in which the original training data has already been split into training and validation with almost equal representation among all classes, and unzip it in the current directory\n",
        "!gdown --id 11SGStqp8Vug2GDzSpJDwQYHThLIjZFQn\n",
        "!unzip -q inaturalist_12K.zip\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tinaturalist_12K  inaturalist_12K.zip  sample_data\n",
            "CPU times: user 2.61 s, sys: 430 ms, total: 3.04 s\n",
            "Wall time: 8min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j52r8UJYXr1U"
      },
      "source": [
        "# !pip install split-folders"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHtrVxShEOOa"
      },
      "source": [
        "# import splitfolders\n",
        "\n",
        "# The code below was used for splitting the training data into training and validation set with equal representations from each class\n",
        "# ---------------------------------- Uncomment below code to split folders once again / check ----------------------------------\n",
        "\n",
        "# splitfolders.ratio('./inaturalist_12K/train', output='./inaturalist_12K', seed=1337, ratio=(.9, .1), group_prefix=None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cjlDvASXtfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb28a2e6-6be4-40cd-e972-437309ec5053"
      },
      "source": [
        "import os\n",
        "def print_count_classes(data_path = './inaturalist_12K/val'):\n",
        "  # Function to check if the images in validation and training set contain nearly equal images belonging to each class\n",
        "  class_count_valid = {}\n",
        "  for subdir, dirs, files in os.walk(data_path):\n",
        "      for file in files:\n",
        "        class_count_valid[subdir] = class_count_valid.get(subdir,0)+1\n",
        "\n",
        "  print(f'In path {data_path} : {class_count_valid}')\n",
        "\n",
        "# --------------------------- Uncomment below code to check count of images in each class in training and validation set ----------------------------\n",
        "\n",
        "# print_count_classes('./inaturalist_12K/val')\n",
        "# print_count_classes('./inaturalist_12K/train')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgiGx6yCuB-Q",
        "outputId": "c0bb0cf8-c6a0-40ef-a93f-1fa915fc8821"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(physical_devices))\n",
        "\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdy2ocN-wnil"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_model_partA(inp_img_shape, K_list, F_list, no_neurons_dense, no_classes = 10, pooling_list = ['max']*5, activation_fn_list = ['relu']*6, \n",
        "                      P_list = ['valid']*10, S_list = [1]*10, reg_list = ['none']*7, lambda_ = 0.001, BN_yes = False, dropout_p = 0):\n",
        "    '''\n",
        "    Function to build the model comprising (5 conv+relu+maxpooling layers + 1 dense FC layer) for part A in keras\n",
        "    Arguments :\n",
        "        inp_img_shape -- shape of input image\n",
        "        K_list -- List of number of filters in each non FC layer\n",
        "        F_list -- List of size of filters (assumed same dimension in width and height) in each non FC layer  \n",
        "        no_neurons_dense -- Number of neurons in the dense FC layer\n",
        "        no_classes -- Number of output classes in the classification problem\n",
        "        pooling_list -- List of pooling layer option for each conv+pooling block ('max' : MaxPooling2D, 'avg': AveragePooling2D)\n",
        "        activation_fn_list -- List of activation function in each convolution layer and the onne hidden FC layer\n",
        "        P_list -- List of padding options in each non FC layer \n",
        "                  ('valid' : no padding, 'same' : padding to make input and output same dimensions)\n",
        "        S_list -- List of strides (assumed equal in width and height) in each non FC layer\n",
        "        reg_list -- List of regularization options for the convolution, one hidden FC and output layers ('none' : no regularization, 'L2' , 'L1')\n",
        "        lambda_ -- weight decay hyperparameter for regularisation\n",
        "        BN_yes -- True : Batch normalisation (BN) should be used, False : BN should not be used\n",
        "        dropout_p -- Probability of dropping out a neuron\n",
        "                     (The dropout is added for the single dense hidden layer alone after referring to many CNN architecture papers)\n",
        "\n",
        "    Returns :\n",
        "        model -- The keras sequential model of the CNN created\n",
        "    '''\n",
        "    get_regularization = {\n",
        "        'none': None,\n",
        "        'L1': regularizers.l1(lambda_),\n",
        "        'L2': regularizers.l2(lambda_)\n",
        "    }\n",
        "\n",
        "    get_pooling_layer = {\n",
        "        'max': MaxPooling2D,\n",
        "        'avg': AveragePooling2D\n",
        "    }\n",
        "\n",
        "    model = Sequential()\n",
        "    # First layer\n",
        "    model.add(Conv2D(filters = K_list[0], kernel_size = (F_list[0], F_list[0]), strides = (S_list[0], S_list[0]), \n",
        "                     padding = P_list[0], input_shape = inp_img_shape, kernel_regularizer = get_regularization[reg_list[0]]))\n",
        "    if BN_yes:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activation_fn_list[0]))\n",
        "    model.add(get_pooling_layer[pooling_list[0]](pool_size=(F_list[1], F_list[1]), strides = (S_list[1], S_list[1]), padding = P_list[1]))\n",
        "\n",
        "    # 4 Conv-relu-MaxPooling layers\n",
        "    for l in range(1, 5):\n",
        "        model.add(Conv2D(filters = K_list[2*l], kernel_size = (F_list[2*l], F_list[2*l]), strides = (S_list[2*l], S_list[2*l]), \n",
        "                         padding = P_list[2*l], kernel_regularizer = get_regularization[reg_list[l]]))\n",
        "        if BN_yes:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(Activation(activation_fn_list[l]))\n",
        "        model.add(get_pooling_layer[pooling_list[l]](pool_size = (F_list[2*l+1], F_list[2*l+1]), strides = (S_list[2*l+1], S_list[2*l+1]), padding = P_list[2*l+1]))\n",
        "    \n",
        "    # 1 dense FC layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout_p))\n",
        "    model.add(Dense(units = no_neurons_dense, kernel_regularizer = get_regularization[reg_list[5]]))\n",
        "    if BN_yes:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activation = activation_fn_list[5]))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(units = no_classes, kernel_regularizer = get_regularization[reg_list[6]]))\n",
        "    if BN_yes:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activation = 'softmax'))\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgmFIPE_DL5",
        "outputId": "98dca65a-7b5f-4f95-df4d-0a836f1878c9"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Using cached wandb-0.10.25-py2.py3-none-any.whl (2.1 MB)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Using cached sentry_sdk-1.0.0-py2.py3-none-any.whl (131 kB)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in ./venvA/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
            "Collecting Click>=7.0\n",
            "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "Processing /Users/abishek_programming/Library/Caches/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2/pathtools-0.1.2-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in ./venvA/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
            "Processing /Users/abishek_programming/Library/Caches/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d/promise-2.3-py3-none-any.whl\n",
            "Processing /Users/abishek_programming/Library/Caches/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a/subprocess32-3.5.4-py3-none-any.whl\n",
            "Collecting psutil>=5.0.0\n",
            "  Using cached psutil-5.8.0-cp37-cp37m-macosx_10_9_x86_64.whl (236 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Using cached shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in ./venvA/lib/python3.7/site-packages (from wandb) (3.15.8)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in ./venvA/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Using cached configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Using cached GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "Collecting PyYAML\n",
            "  Using cached PyYAML-5.4.1-cp37-cp37m-macosx_10_9_x86_64.whl (249 kB)\n",
            "Requirement already satisfied, skipping upgrade: certifi in ./venvA/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in ./venvA/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (1.26.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in ./venvA/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in ./venvA/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Using cached gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Using cached smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Installing collected packages: sentry-sdk, Click, pathtools, docker-pycreds, promise, subprocess32, psutil, shortuuid, configparser, smmap, gitdb, GitPython, PyYAML, wandb\n",
            "Successfully installed Click-7.1.2 GitPython-3.1.14 PyYAML-5.4.1 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 promise-2.3 psutil-5.8.0 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.25\n",
            "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/Users/abishek_programming/Desktop/CS6910_Assignment2/partA/src/venvA/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/abishek_programming/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T99QShvaRdXa"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Model for resizing and rescaling images\n",
        "image_rescale = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Model for performing random transformations for data augmentation\n",
        "data_augmentation = Sequential([\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),\n",
        "    layers.experimental.preprocessing.RandomZoom(0.2, 0.2),\n",
        "    layers.experimental.preprocessing.RandomContrast(0.2)\n",
        "])\n",
        "\n",
        "def prepare_data(data_path, inp_img_shape, batch_size, img_preprocess, data_augmentation, data_augment_yes = False, shuffle = True):\n",
        "    # Function to generate image data after shuffling and forming batches, and applying data augmentation techniques to it randomly\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    dataset = image_dataset_from_directory(\n",
        "        data_path, labels='inferred', color_mode='rgb', batch_size=batch_size, image_size=inp_img_shape[:-1], shuffle=shuffle,\n",
        "        seed=123, label_mode='categorical'\n",
        "    )\n",
        "    \n",
        "    dataset = dataset.map(lambda x, y: (img_preprocess(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Use data augmentation only if data_augment_yes == True (Training set only requires data augmentation)\n",
        "    if data_augment_yes:\n",
        "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Use buffered prefecting on datasets\n",
        "    return dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "def data_generator(inp_img_shape, batch_size, data_augment_yes = False, train_data_path = None, val_data_path = None, test_data_path = None):\n",
        "    # Function to generate training, validation and test data\n",
        "    train_data = None\n",
        "    if train_data_path is not None:\n",
        "        train_data = prepare_data(train_data_path, inp_img_shape, batch_size, image_rescale, data_augmentation, data_augment_yes)\n",
        "    val_data = None\n",
        "    if val_data_path is not None:\n",
        "        val_data = prepare_data(val_data_path, inp_img_shape, batch_size, image_rescale, data_augmentation, False)\n",
        "    test_data = None\n",
        "    if test_data_path is not None:\n",
        "        test_data = prepare_data(test_data_path, inp_img_shape, batch_size, image_rescale, data_augmentation, False)\n",
        "    \n",
        "    return train_data, val_data, test_data\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COdHx5_sFEAr"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eOL2KSnGffH"
      },
      "source": [
        "def train_model(model, train_data, loss_function, optimizer = 'adam', learning_rate = 1e-3, epochs = 10, val_data = None):\n",
        "    # Function to train the model using the mentioned optimizer, learning rate and epochs\n",
        "    if optimizer == 'adam':\n",
        "        model.compile(optimizer = Adam(learning_rate=learning_rate), loss = loss_function, metrics = ['accuracy'])\n",
        "    elif optimizer == 'momentum':\n",
        "        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9), loss = loss_function, metrics = ['accuracy'])\n",
        "    elif optimizer == 'rmsprop':\n",
        "        model.compile(optimizer = RMSprop(learning_rate=learning_rate), loss = loss_function, metrics = ['accuracy'])\n",
        "    elif optimizer == 'nesterov':\n",
        "        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9, nesterov = True), loss = loss_function, metrics = ['accuracy'])\n",
        "    elif optimizer == 'nadam':\n",
        "        model.compile(optimizer = Nadam(learning_rate=learning_rate), loss = loss_function, metrics = ['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer = SGD(learning_rate=learning_rate), loss = loss_function, metrics = ['accuracy'])\n",
        "\n",
        "    assert(val_data is not None)\n",
        "    model.fit(train_data,\n",
        "              epochs = epochs, \n",
        "              validation_data = val_data,\n",
        "              verbose = 2,\n",
        "              callbacks = [WandbCallback(monitor='val_accuracy'), EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "    # Using validation accuracy as the metric to monitor as that is what is intended to be maximized\n",
        "    return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TObFzZAjISbW"
      },
      "source": [
        "def get_klist(start1, factor):\n",
        "    # Function to get list of number of filters in each convolution layer\n",
        "    start = start1\n",
        "    vals = []\n",
        "    for i in range(5):\n",
        "        vals.append(start)\n",
        "        vals.append(start)\n",
        "        start = max(int(start*factor), 1)\n",
        "    return vals"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km5FPE73117I"
      },
      "source": [
        "def test_model(model, test_data):\n",
        "    # Function to get test accuracy and loss for a model on a test data\n",
        "    assert(test_data is not None)\n",
        "    test_loss, test_accuracy = model.evaluate(test_data, use_multiprocessing = True, workers = 4)\n",
        "    test_accuracy = round(test_accuracy*100, 2)\n",
        "    test_loss = round(test_loss, 4)\n",
        "    print(f'Test Accuracy : {test_accuracy} | Test Loss : {test_loss}')\n",
        "\n",
        "    return test_loss, test_accuracy\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2In7LU6GaOr"
      },
      "source": [
        "def CNN_train(inp_img_shape, train_data_path, K_list, F_list, config, no_classes = 10, pooling_list = ['max']*5, activation_fn_list = ['relu']*6, \n",
        "              P_list = ['valid']*10, S_list = [1]*10, reg_list = ['none']*7, val_data_path = None, test_data_path = None, \n",
        "              wandb_init = True, load_run = None):\n",
        "    '''\n",
        "    Function to train, validate and test a CNN with specific hyperparameters (and architecture as mentioned in the question), \n",
        "    or test a CNN which was already trained.\n",
        "    (NOTE : the function uses WANDB to log the best model and training metrics)\n",
        "    \n",
        "    Arguments :\n",
        "        inp_img_shape -- (tuple) shape of input image\n",
        "        train_data_path -- (string) the path to training data\n",
        "        K_list -- (list) List of number of filters in each non FC layer\n",
        "        F_list -- (list) List of size of filters (assumed same dimension in width and height) in each non FC layer \n",
        "        config -- (dictionary) contains all the hyperparameter and architectural configurations used for the model \n",
        "                  [refer to the config_1 in next cell to see what all it contains]\n",
        "        no_classes -- (int) Number of output classes in the classification problem\n",
        "        pooling_list -- (list) List of pooling layer option for each conv+pooling block ('max' : MaxPooling2D, 'avg': AveragePooling2D)\n",
        "        activation_fn_list -- (list) List of activation function in each convolution layer and the onne hidden FC layer\n",
        "        P_list -- (list) List of padding options in each non FC layer \n",
        "                  ('valid' : no padding, 'same' : padding to make input and output same dimensions)\n",
        "        S_list -- (list) List of strides (assumed equal in width and height) in each non FC layer\n",
        "        reg_list -- (list) List of regularization options for the convolution, one hidden FC and output layers ('none' : no regularization, 'L2' , 'L1')\n",
        "        val_data_path -- (string) the path to validation data\n",
        "        test_data_path -- (string) the path to test data\n",
        "        wandb_init -- (bool) True : WANDB run has been initiated outside the function | False : WANDB run not initiated\n",
        "        load_run -- (string) WANDB run ID to restore and use a previously trained model (None to create a new model)\n",
        "\n",
        "    Returns :\n",
        "        model -- (Keras Model object) the CNN model which was used for training and/or testing\n",
        "        id -- (string) the unique run ID from WANDB\n",
        "    '''\n",
        "    id = ''\n",
        "    if wandb_init:\n",
        "        id = wandb.util.generate_id()\n",
        "        run = wandb.init(id = id, project=\"assignment2\", entity=\"abisheks\", reinit=True, config=config)\n",
        "        \n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    if load_run is None:\n",
        "        model = build_model_partA(inp_img_shape, K_list, F_list, config['no_neurons_dense'], no_classes, pooling_list, activation_fn_list, \n",
        "                                  P_list, S_list, reg_list, config['weight_decay'], config['batch_normalization'], config['dropout'])\n",
        "    else:\n",
        "        api = wandb.Api()\n",
        "        run_prev = api.run('abisheks/assignment2/'+load_run)\n",
        "        prev_model_file = run_prev.file('model-best.h5').download(replace=True)\n",
        "        model = tf.keras.models.load_model(prev_model_file.name)\n",
        "\n",
        "    assert(train_data_path is not None)\n",
        "    train_data, val_data, test_data = data_generator(inp_img_shape, config['batch_size'], config['data_augmented'], \n",
        "                                                     train_data_path, val_data_path, test_data_path)\n",
        "    model = train_model(model, train_data, config['loss_function'], config['optimizer'], config['learning_rate'], config['epochs'], val_data)\n",
        "    \n",
        "    if test_data is not None:\n",
        "        test_loss, test_accuracy = test_model(model, test_data)\n",
        "        wandb.log({'test_accuracy': test_accuracy, 'test_loss': test_loss})\n",
        "\n",
        "    if wandb_init:\n",
        "        run.finish()\n",
        "\n",
        "    return model, id\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isIxc_ytIV_G"
      },
      "source": [
        "# Hyperparameters for building the model for Part-A\n",
        "K_list_1 = [32, 32, 32, 32, 64, 64, 64, 64, 128, 128]           # List of number of filters in each non FC layer\n",
        "F_list_1 = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]                       # List of size of filters in each non FC layer  \n",
        "activation_fn_list_1 = ['relu']*6                               # List of activation function in each convolution and FC layer\n",
        "P_list_1 = ['valid']*10                                         # List of padding options in each non FC layer ('valid' : no padding, 'same' : padding to make input and output same dimensions)\n",
        "S_list_1 = [1, 2, 1, 2, 1, 2, 1, 2, 1, 1]                       # List of number of strides in each non FC layer\n",
        "reg_list_1 = ['L2', 'L2', 'L2', 'L2', 'L2', 'L2', 'L2']         # List of regularization options for the convolution, one hidden FC and output layers ('none' : no regularization, 'L2' , 'L1')\n",
        "inp_img_shape_1 = (227, 227, 3)                                 # Shape of input image from data\n",
        "no_classes_1 = 10                                               # Number of output classes in the classification problem\n",
        "pooling_list_1 = ['max']*4 + ['avg']                            # List of pooling layer option for each conv+pooling block ('max' : MaxPooling2D, 'avg': AveragePooling2D)\n",
        "\n",
        "# The dictionary that stores all hyperparameter and architectural information about the model. Will be sent to wandb to describe the model in the run\n",
        "config_1 = {\n",
        "    \"learning_rate\": 1e-3,                                      # Hyperparameter for updating the parameters in gradient descent\n",
        "    \"epochs\": 10,                                               # Number of epochs to train the model   \n",
        "    \"optimizer\": 'nesterov',                                    # Gradient descent algorithm used for the parameter updation\n",
        "    \"batch_size\": 64,                                           # Batch size used for the optimizer\n",
        "    \"loss_function\": 'categorical_crossentropy',                # Loss function used in the optimizer\n",
        "    \"architecture\": 'CNN',                                      # Type of neural network used\n",
        "    \"dataset\": \"iNaturalist_12K\",                               # Name of dataset\n",
        "    'no_filters': 32,                                           # Number of filters for the first convolution layer\n",
        "    'filter_organization': 1,                                   # The factor by which the number of filters change in the subseqeuent convolution layers\n",
        "    'no_neurons_dense': 64,                                     # Number of neurons in the dense FC layer\n",
        "    'data_augmented': False,                                    # True : Data augmentation is done during training, False : No data augmentation done\n",
        "    'dropout' : 0.2,                                            # Probability of dropping out a neuron in dropout technique\n",
        "    'batch_normalization': True,                                # True : Batch normalisation (BN) should be used, False : BN should not be used\n",
        "    'weight_decay': 0.01,                                       # weight decay hyperparameter for regularization\n",
        "    'F_list': F_list_1,\n",
        "    'activation_fn_list': activation_fn_list_1,\n",
        "    'P_list': P_list_1,\n",
        "    'S_list': S_list_1,\n",
        "    'regularization_list': reg_list_1,\n",
        "    'input_image_shape': inp_img_shape_1,\n",
        "    'pooling_layer_list': pooling_list_1\n",
        "}\n",
        "\n",
        "\n",
        "# PART-A, Question 1 -- Building a model with (5 conv+relu+maxpooling layers + 1 dense FC layer) for image classification objective\n",
        "# ---------------------------------- To test the CNN_train function uncomment the code below ----------------------------------\n",
        "\n",
        "# modelA, _ = CNN_train(inp_img_shape_1, './inaturalist_12K/train', get_klist(32, 1.5), F_list_1, config_1, no_classes_1, \n",
        "#                       pooling_list_1, activation_fn_list_1, P_list_1, S_list_1, reg_list_1, './inaturalist_12K/val', './inaturalist_12K/test')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwVfR6wfthS"
      },
      "source": [
        "# Hyperparameter choices to sweep \n",
        "sweep_config = {\n",
        "    'name': 'CNN',\n",
        "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'no_filters': {\n",
        "            'values': [32, 64]\n",
        "        },\n",
        "        'filter_organization': {\n",
        "            'values': [1, 1.5, 2]\n",
        "        },\n",
        "        'data_augmented': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'dropout' :{\n",
        "            'values': [0, 0.25, 0.4]\n",
        "        },\n",
        "        'batch_normalization': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'no_neurons_dense': {\n",
        "            'values': [32, 64, 256]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nesterov']\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0.01, 0.001]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnCtc0c2eMqR"
      },
      "source": [
        "def sweep_wrapper(data_path = './inaturalist_12K'):\n",
        "    # Wrapper function to call the CNN function for sweeping with different hyperparameters\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    run = wandb.init(config=config_1, reinit=True)\n",
        "\n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = f'nf_{config.no_filters}_fo_{config.filter_organization}_dr_{config.dropout}'\n",
        "    wandb.run.name += '_da' if config.data_augmented else '' \n",
        "    wandb.run.name += '_bn' if config.batch_normalization else ''\n",
        "    wandb.run.save()\n",
        "    print(wandb.run.name)\n",
        "\n",
        "    modelA, _ = CNN_train(inp_img_shape_1, f'{data_path}/train', get_klist(config.no_filters, config.filter_organization), F_list_1,\n",
        "                          config, no_classes_1, pooling_list_1, activation_fn_list_1, P_list_1, S_list_1, reg_list_1, \n",
        "                          f'{data_path}/val', wandb_init = False)\n",
        "    run.finish()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL25cORLeRto"
      },
      "source": [
        "# PART-A, Question 2 -- Sweeping across different sets of hyperparameters\n",
        "# ---------------------------------- To run the sweep uncomment the code below ----------------------------------\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment2\")\n",
        "# wandb.agent(sweep_id, lambda : sweep_wrapper())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdM_119RJ_ai"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Function to process the tensor to visualize it as an image\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    \n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t75Tu8EmjJOJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b7bf35725e3c48f6a048ea6c24a1f45a",
            "fa92da97ab1a44979c7bf3ef5c58defb",
            "b30f66d09ce74efcb9384a7cb41cfed7",
            "ca0c49807c11404f8735bdc3b52d29f3",
            "af7da6bc2e984a129374e591262209ae",
            "9c76c920cc36428494cc0c236553058b",
            "b9c699f7a6db416caa141c789361e4da",
            "e8f25d64ad4c42cd8bfe254fab281ee3"
          ]
        },
        "outputId": "7e9e4f4d-3075-419d-8f33-46f414f5acc0"
      },
      "source": [
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "def analyze_best_model(wandb_log = False):\n",
        "    '''\n",
        "    Function to evaluate the best model for Part A on test set, plot sample test results and visualize the filters and their outputs in first layer.\n",
        "    Arguments :\n",
        "        wandb_log -- (bool) True : log the plots and results in WANDB | False : do not log them to WANDB, visualize here alone\n",
        "    Returns :\n",
        "        -- None --\n",
        "    '''\n",
        "    best_run_path = 'abisheks/assignment2/huyhhsb4'\n",
        "    api = wandb.Api()\n",
        "    run = api.run(best_run_path)\n",
        "    model_file = run.file('model-best.h5').download(replace=True)\n",
        "    model = tf.keras.models.load_model(model_file.name)\n",
        "    config_file = run.file('config.yaml').download(replace=True)\n",
        "    with open(config_file.name, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "\n",
        "    # Generate test_data and find the test accuracy of the model\n",
        "    _, _, test_data = data_generator(config['input_image_shape']['value'], config['batch_size']['value'], test_data_path = './inaturalist_12K/test')\n",
        "    test_loss, test_accuracy = test_model(model, test_data)\n",
        "    if wandb_log:\n",
        "        run = wandb.init(project=\"assignment2\", entity=\"abisheks\", reinit=True)\n",
        "        wandb.log({'test_loss': test_loss, 'test_accuracy': test_accuracy})\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    # String labels for the different classes\n",
        "    class_labels = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n",
        "    class_labels = sorted(class_labels)\n",
        "\n",
        "    # Choose 30 random test images to show the true and predicted class in a 10 x 3 grid\n",
        "    random_sample_paths = random.sample(glob.glob('./inaturalist_12K/test/*/*'), 30)\n",
        "    inp_samples = [np.array(Image.open(sample_path).resize(config['input_image_shape']['value'][:-1])) for sample_path in random_sample_paths]\n",
        "    true_labels = [os.path.normpath(sample_path).split(os.path.sep)[-2] for sample_path in random_sample_paths]\n",
        "    predicted_class_nos = np.argmax(model.predict(np.array(inp_samples)), axis=-1)\n",
        "    predicted_labels = [class_labels[class_no] for class_no in predicted_class_nos]\n",
        "    \n",
        "    R, C = 10, 3\n",
        "    fig, ax = plt.subplots(R, C, figsize=(15, 18))\n",
        "    fig.suptitle(\"Model predictions for 30 sample images\", fontsize='x-large')\n",
        "    for i in range(R):\n",
        "        for j in range(C):\n",
        "            idx = i*C + j\n",
        "            ax[i][j].imshow(Image.open(random_sample_paths[idx]).resize((500, 500)))\n",
        "            ax[i][j].axis('off')\n",
        "            ax[i][j].text(1.1, 0.7, \"True : \", size='large', ha=\"left\", va='center', color='black', transform=ax[i][j].transAxes)\n",
        "            ax[i][j].text(1.45, 0.7, true_labels[idx], size='large', ha=\"left\", va='center', color='blue', transform=ax[i][j].transAxes)\n",
        "            pred_color = 'green' if true_labels[idx] == predicted_labels[idx] else 'red'\n",
        "            ax[i][j].text(1.1, 0.3, \"Predicted : \", size='large', ha=\"left\", va='center', color='black', transform=ax[i][j].transAxes)\n",
        "            ax[i][j].text(1.75, 0.3, predicted_labels[idx], size='large', ha=\"left\", va='center', color=pred_color, transform=ax[i][j].transAxes)\n",
        "\n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    if wandb_log:\n",
        "        wandb.log({'sample_predictions': fig})\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        " \n",
        "    # Choose a random image from the test data and visualize the filters and output in first layer for that image\n",
        "    random_image_path = random.sample(glob.glob('./inaturalist_12K/test/*/*'), 1)[0]\n",
        "    rand_inp_image = np.array(Image.open(random_image_path).resize(config['input_image_shape']['value'][:-1]))\n",
        "    \n",
        "    # The first convolution layer of the model is named 'conv2d'\n",
        "    layer = model.get_layer(name = 'conv2d')\n",
        "    # Forming an intermediate keras model to get the output of the first layer in suitable format to visulaize it\n",
        "    intermediate_model = tf.keras.Model(inputs=model.input, outputs=layer.output)\n",
        "    layer_outputs = intermediate_model.predict(rand_inp_image.reshape(1, *rand_inp_image.shape))\n",
        "    # Getting layer weights\n",
        "    layer_weights = layer.weights\n",
        "\n",
        "    fig_img = plt.figure()\n",
        "    plt.title('Random Image used for visualizing filters', fontsize='x-large')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.array(Image.open(random_image_path)))\n",
        "\n",
        "    R, C = int(config['no_filters']['value'] / 8), 8\n",
        "    fig1, ax1 = plt.subplots(R, C, figsize=(15, 15))\n",
        "    fig2, ax2 = plt.subplots(R, C, figsize=(15, 15))\n",
        "    fig1.suptitle(f\"Visualizing output of {config['no_filters']['value']} filters in first layer\", fontsize='x-large')\n",
        "    fig2.suptitle(f\"Visualizing {config['no_filters']['value']} filters in first layer\", fontsize='x-large')\n",
        "    for i in range(R):\n",
        "        for j in range(C):\n",
        "            idx = i*C + j\n",
        "            ax1[i][j].set_title(f'Filter : {idx+1}')\n",
        "            ax1[i][j].axis('off')\n",
        "            ax2[i][j].set_title(f'Filter : {idx+1}')\n",
        "            ax2[i][j].axis('off')\n",
        "            ax1[i][j].imshow(deprocess_image(layer_outputs[0][:,:,idx]), cmap='viridis')\n",
        "            ax2[i][j].imshow(deprocess_image(layer_weights[0].numpy()[:,:,:,idx]))\n",
        "    \n",
        "    fig1.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    fig2.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    if wandb_log:\n",
        "        wandb.log({'random_image': wandb.Image(Image.open(random_image_path))})\n",
        "        wandb.log({'filters': fig2})\n",
        "        wandb.log({'filter_outputs': fig1})\n",
        "        run.finish()\n",
        "\n",
        "    fig1.show()\n",
        "    fig2.show()\n",
        "\n",
        "# PART-A, Question 4 -- Analyzing the best model for part A and visualize the filters in 1st layer\n",
        "# ---------------------------------- To run the analyze function uncomment the code below ----------------------------------\n",
        "\n",
        "# analyze_best_model()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebkh3kAslcgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "734ecba5fb41499291f655ace9f9828f",
            "4783c1e69b734c8a806ad7578e3a7253",
            "f6cbf123491645ad9f348c1c70a3ebcc",
            "35872fd3056648109211cbeedd73ba70",
            "7c1226ec8e144d768d18d8028399658f",
            "e3caf048d4e24c3aadb9ba6f112d375d",
            "9da06116257d40478134e605d95a5a9b",
            "876bd581e480419da6ca48a23d325ff1"
          ]
        },
        "outputId": "2fd644dc-2b7f-4efe-f750-b7e87874d482"
      },
      "source": [
        "def guided_backprop(wandb_log = False):\n",
        "    '''\n",
        "    Function to do guided backpropogation and visualize results for the CONV-5 layer.\n",
        "    Arguments :\n",
        "        wandb_log -- (bool) True : log the plots and results in WANDB | False : do not log them to WANDB, visualize here alone\n",
        "    Returns :\n",
        "        -- None --\n",
        "    '''\n",
        "    @tf.custom_gradient\n",
        "    def guidedRelu(x):\n",
        "        def grad(dy):\n",
        "            return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
        "        return tf.nn.relu(x), grad\n",
        "\n",
        "    best_run_path = 'abisheks/assignment2/huyhhsb4'\n",
        "    api = wandb.Api()\n",
        "    run = api.run(best_run_path)\n",
        "    model_file = run.file('model-best.h5').download(replace=True)\n",
        "    model = tf.keras.models.load_model(model_file.name)\n",
        "    config_file = run.file('config.yaml').download(replace=True)\n",
        "    with open(config_file.name, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "\n",
        "    random_image_path = random.sample(glob.glob('./inaturalist_12K/test/*/*'), 1)[0]\n",
        "    rand_inp_image = np.array(Image.open(random_image_path).resize(config['input_image_shape']['value'][:-1]))\n",
        "\n",
        "    fig_img = plt.figure()\n",
        "    plt.title('Random Image used for guided backpropogation', fontsize='x-large')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(np.array(Image.open(random_image_path).resize(config['input_image_shape']['value'][:-1])))\n",
        "\n",
        "    if wandb_log:\n",
        "        run = wandb.init(project=\"assignment2\", entity=\"abisheks\", reinit=True)\n",
        "        wandb.log({'random_image': wandb.Image(Image.open(random_image_path))})\n",
        "\n",
        "    gb_model = tf.keras.Model(\n",
        "        inputs = [model.inputs],\n",
        "        outputs = [model.get_layer(\"conv2d_4\").output]\n",
        "    )\n",
        "    output_shape = model.get_layer(\"conv2d_4\").output.shape[1:]\n",
        "    layer_dict = [layer for layer in gb_model.layers if hasattr(layer,'activation')]\n",
        "    for layer in layer_dict:\n",
        "        if layer.activation == tf.keras.activations.relu:\n",
        "            layer.activation = guidedRelu\n",
        "    \n",
        "    R, C = 10, 1\n",
        "    fig, ax = plt.subplots(R, C, figsize=(15, 28))\n",
        "    fig.suptitle(\"Visualizing gradient for 10 neurons in CONV-5 layer\", fontsize='x-large')\n",
        "    for i in range(10):\n",
        "        rand_neuron_index = [0] + [random.randint(0, dim_max-1) for dim_max in output_shape]\n",
        "\n",
        "        mask_mat = np.zeros((1, *output_shape))\n",
        "        mask_mat[rand_neuron_index[0], rand_neuron_index[1], rand_neuron_index[2], rand_neuron_index[3]] = 1\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            inputs = tf.cast(rand_inp_image.reshape(1, *rand_inp_image.shape), tf.float32)\n",
        "            tape.watch(inputs)\n",
        "            outputs = gb_model(inputs) * mask_mat\n",
        "\n",
        "        grad = tape.gradient(outputs,inputs)[0]\n",
        "\n",
        "        ax[i].set_title(f'Neuron-{i+1} index : {tuple(rand_neuron_index[1:])}')\n",
        "        ax[i].axis('off')\n",
        "        ax[i].imshow(deprocess_image(np.array(grad)))\n",
        "    \n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    if wandb_log:\n",
        "        wandb.log({'guided_backprop_10_neurons_visualization': fig})\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        inputs = tf.cast(rand_inp_image.reshape(1, *rand_inp_image.shape), tf.float32)\n",
        "        tape.watch(inputs)\n",
        "        outputs = gb_model(inputs)\n",
        "\n",
        "    grad = tape.gradient(outputs,inputs)[0]\n",
        "    fig_whole = plt.figure()\n",
        "    plt.title('Visualizing gradient for whole CONV-5 layer', fontsize='x-large')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(deprocess_image(np.array(grad)))\n",
        "    plt.show()\n",
        "\n",
        "    if wandb_log:\n",
        "        wandb.log({'guided_backprop_whole_visualization': fig_whole})\n",
        "        run.finish()\n",
        "\n",
        "# PART-A, Question 5 -- Guided backpropogation in CONV-5 layer\n",
        "# ---------------------------------- To run the guided backprop function uncomment the code below ----------------------------------\n",
        "\n",
        "# guided_backprop()"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}